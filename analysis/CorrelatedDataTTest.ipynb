{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Consensus analysis using mean Z score\n",
    "import numpy\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Tom:\n",
    "\n",
    "Following on our chat, I realise that my notes were missing some indices. Here’s a corrected version:\n",
    "\n",
    "\n",
    "    Y_i: N-vector of T scores (or whatever) at voxel i\n",
    "    Var(Y_i) =  Sigma_i\n",
    "\n",
    "where Sigma_i is the NxN covariance matrix. But to be practical, we need to assume common variance, and a global correlation:\n",
    "\n",
    "    Var(Y_i) =  sigma_i Q\n",
    "\n",
    "Where sigma_i is the (scalar) variance at voxel i, Q the common NxN correlation (*not* covariamce)\n",
    "\n",
    "Then the average is \n",
    "    bar{Y_i} = X’ Y_i/N\n",
    "and\n",
    "    Var(bar{Y_i}) =  sigma^2_i   X’ Q X / N^2\n",
    "\n",
    "So then the T test is \n",
    "   T_i = bar(Y_i) / sqrt(Var(bar{Y_i}))\n",
    "\n",
    "If you estimate the variance as you suggest\n",
    "\n",
    "   hat{sigma^2_i} = Y_i’ R Y_i / tr(RQ)\n",
    "\n",
    "then the effective DF is as you say,\n",
    "\n",
    "    v = tr(RQ)^2 / tr(RQRQ)\n",
    "\n",
    "But you could also use the naive estimate\n",
    "\n",
    "   hat{sigma^2_i} = Y_i’ R Y_i / (N-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def t_corr(y,Q=None):\n",
    "    \"\"\"\n",
    "    perform a one-sample t-test on correlated data\n",
    "    y = data (n observations X n vars)\n",
    "    Q = \"known\" correlation across observations (use empirical correlation based on maps)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Jeanette:\n",
    "    # This paper calculates the df for an F-test, so the chisquare bit we need is in there.  Your t-statistic will come from\n",
    "    # X = column of 1's (design matrix)\n",
    "\n",
    "    X = numpy.ones((npts,1))\n",
    "\n",
    "    if Q is None:\n",
    "        #print('no Q specified, using identity (uncorrelated)')\n",
    "        Q = numpy.eye(npts)\n",
    "\n",
    "    # R = I{n} - X(X'X)^{-1}X'\n",
    "    R = numpy.eye(npts) - X.dot(numpy.linalg.inv(X.T.dot(X))).dot(X.T)\n",
    "\n",
    "    # s-hat-2 = y'Ry/tr(RQ)\n",
    "    s_hat_2 = y.T.dot(R).dot(y)/(numpy.trace(R.dot(Q)))\n",
    "\n",
    "    # T  =  mean(y,0)/s-hat-2\n",
    "    # use diag to get s_hat2 for each variable \n",
    "    T = numpy.mean(y,0)/numpy.diag(s_hat_2)\n",
    "\n",
    "    # degrees of freedom = v = tr(RQ)^2/tr(RQRQ)\n",
    "    df = (numpy.trace(R.dot(Q))**2)/numpy.trace(R.dot(Q).dot(R).dot(Q))\n",
    "    p = scipy.stats.t.cdf(T,df=df)\n",
    "    return(T,df,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 36\n",
    "nvars = 10\n",
    "nruns=1000\n",
    "\n",
    "# simulate independent case\n",
    "pvals = []\n",
    "for i in range(nruns):\n",
    "    y = numpy.random.randn(npts,nvars)\n",
    "    result = t_corr(y)\n",
    "    pvals.append(result[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0499632546504537"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvals_mtx = numpy.array(pvals)\n",
    "numpy.mean(pvals_mtx/nvars)   # bonferroni correction, should come out around 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate correlated data\n",
    "\n",
    "def mk_correlated_data(npts,nvars,r):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Tom:\n",
    "# Yup... that’s the direction, but need to work out the variance of the mean too, not just worry about DF:\n",
    "\n",
    "# So...\n",
    "\n",
    "#     Y_i: N-vector of T scores (or whatever) at voxel i\n",
    "#     Var(Y_i) =  Sigma_i\n",
    "\n",
    "# where Sigma_i is the NxN covariance matrix. But to be practical, we need to assume common variance, and a global correlation:\n",
    "\n",
    "#     Var(Y_i) =  sigma_i Q\n",
    "\n",
    "# Where sigma_i is the (scalar) variance at voxel i, Q the common correlation\n",
    "\n",
    "# Then the average is \n",
    "#     bar{Y_i} = X’Y_i/N\n",
    "# and\n",
    "#     Var(bar{Y}) =  sigma^2_i X_i’ Q X_i / N^2\n",
    "\n",
    "# So then the T test is \n",
    "#    T = bar(Y) / sqrt(Var(bar{Y}))\n",
    "\n",
    "# If you estimate the variance as you suggest\n",
    "\n",
    "#    hat{sigma^2_i} = Y’RY / tr(RQ)\n",
    "\n",
    "# then the effective DF is as you say,\n",
    "\n",
    "#     v = tr(RQ)^2 / tr(RQRQ)\n",
    "\n",
    "# But you could also use the naive estimate\n",
    "\n",
    "#    hat{sigma^2_i} = Y’RY / (N-1)\n",
    "\n",
    "# but then the DF are\n",
    "\n",
    "#    v = (N-1)^2 / tr(RQRQ)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
