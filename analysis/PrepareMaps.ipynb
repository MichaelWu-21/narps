{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare maps for analysis\n",
    "\n",
    "#### Hypotheses:\n",
    "\n",
    "Parametric effect of gain:\n",
    "\n",
    "1. Positive effect in ventromedial PFC - for the equal indifference group\n",
    "2. Positive effect in ventromedial PFC - for the equal range group\n",
    "3. Positive effect in ventral striatum - for the equal indifference group\n",
    "4. Positive effect in ventral striatum - for the equal range group\n",
    "\n",
    "Parametric effect of loss:\n",
    "- 5: Negative effect in VMPFC - for the equal indifference group\n",
    "- 6: Negative effect in VMPFC - for the equal range group\n",
    "- 7: Positive effect in amygdala - for the equal indifference group\n",
    "- 8: Positive effect in amygdala - for the equal range group\n",
    "\n",
    "Equal range vs. equal indifference:\n",
    "\n",
    "- 9: Greater positive response to losses in amygdala for equal range condition vs. equal indifference condition.\n",
    "\n",
    "#### Notes on data\n",
    "\n",
    "Data and hypothesis decisions were obtained from 70 teams.  Of these, the maps for 17 teams were excluded from further analysis due to the following problems:\n",
    "\n",
    "##### Bad normalization/resampling:\n",
    "\n",
    "- SVXXBBHN_98BT: rejected due to very bad normalization\n",
    "- UGXECSGG_L1A8: resampled image much smaller than template brain\n",
    "- VDOVGCPL_4SZ2: resampled image offset from template brain\n",
    "- WBTVHMSS_4TQ6: resampled image  offset and too large compared to template\n",
    "\n",
    "\n",
    "##### Missing thresholded images:\n",
    "\n",
    "- WIYTWEEA_2T7P: missing thresholded images\n",
    "\n",
    "\n",
    "##### Used surface-based analysis (only provided data for cortical ribbon:\n",
    "\n",
    "- DRVQPPNO_1K0E\n",
    "- UMSEIVRB_X1Z4 \n",
    "\n",
    "\n",
    "##### Bad unthresholded images:\n",
    "\n",
    "- TMZGNUWI_R5K7: \"unthresholded\" images appeared to be thresholded\n",
    "\n",
    "\n",
    "\n",
    "##### Missing data:\n",
    "\n",
    "- YWGVKXBZ_6FH5: missing much of the central brain\n",
    "- ACMKXFTG_P5F3: rejected due to large amounts of missing data across brain\n",
    "- NNUNPWLT_0H5E: rejected due to large amount of missing brain in center\n",
    "- CMJIFMMR_L3V8: rejected due to large amount of missing brain in center\n",
    "\n",
    "\n",
    "##### Bad histograms:  \n",
    "visual examiniation of histograms of unthresholded images showed a number of them that were clearly not distribted as z/t\n",
    "\n",
    "- RIIVGRDK_E3B6: very long tail, with substantial inflation at a value just below zero\n",
    "- JAWCZRDS_V55J: very small values\n",
    "- EPBGXICO_I07H: bimodal, with second distribution centered around 2.5\n",
    "- UXJVRRRR_0C7Q: appears to be a p-value distribution, with slight excursions below and above zero\n",
    "- OXLAIRNK_Q58J: bimodal, zero-inflated with a second distribution centered around 5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the functionality is now embedded in the Narps() class within narps.py\n",
    "import os,sys,glob,warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import nilearn.input_data\n",
    "\n",
    "from narps import Narps\n",
    "from utils import get_masked_data\n",
    "\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up directories\n",
    "basedir='/Users/poldrack/data_unsynced/NARPS'\n",
    "\n",
    "# instantiate main Narps class, which loads data\n",
    "# to rerun everything, set overwrite to True\n",
    "\n",
    "narps = Narps(basedir,overwrite=overwrite)\n",
    "\n",
    "output_dir = narps.dirs.dirs['output']\n",
    "mask_img = narps.dirs.MNI_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cached data\n",
    "\n",
    "# narps.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create binarized thresholded masks\n",
    "\n",
    "load the thresholded maps and binarize the absolute value (to detect either positive or negative signals).  save output to the *thresh_mask_orig* directory (in original space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.get_binarized_thresh_masks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample images\n",
    "\n",
    "The images were supposed to be submitted in MNI space.  Here we use nilearn.image.resample_to_img() to resample the image into the FSL MNI space (91x109x91, 2mm isotropic).  For unthresholded maps, we use continuous interpolation; for thresholded maps, we use linear interpolation and then threshold at 0.5; this avoids some errors that occurred using nearest neighbor interpolation, where there were empty voxels in some slices. save to *resampled* directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.get_resampled_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check image values\n",
    "\n",
    "Compute number of NA and zero voxels for each image, and save to image_metadata_df.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_value_df = narps.check_image_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine resampled images \n",
    "\n",
    "Combine all images into a single concatenated file for each hypothesis.  save to *{thresh,unthresh}\\_concat\\_resampled*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.create_concat_images(datatype='resampled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectify maps\n",
    "\n",
    "Rectify the unthresholded maps if they used positive values for negative activations - so that all unthresholded maps reflect positive values for the hypothesis in question.  We check this automatically by comparing the thresholded and unthresholded maps - if all voxels within the thresholded map are negative, then we multiply the unthresholded map by -1.  Save to *rectified*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.create_rectified_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create concatenated rectified unthresholded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.create_concat_images(datatype='rectified',imgtypes = ['unthresh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute image statistics\n",
    "\n",
    "Compute range and standard deviation maps across teams for unthresholded/rectified maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.compute_image_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all unthresholded images to Z stat images\n",
    "\n",
    "Use metadata reported by teams (contained in NARPS_analysis_teams_members.csv) which specifies what kind of unthresholded statistical images were submitted.  For those who reported T values, convert those to Z values using Hughett's transform (based on https://github.com/vsoch/TtoZ/blob/master/TtoZ/scripts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narps.convert_to_zscores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate smoothness\n",
    "\n",
    "Use FSL's SmoothEstimate (via nipype) to estimate smoothness of each Z map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothness_df = narps.estimate_smoothness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save outputs\n",
    "\n",
    "Serialize the important information from the class and save to *narps_prepare_maps.pkl*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = narps.write_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reports\n",
    "\n",
    "Create some diagnostic reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make report showing all orig maps with threshold overlays\n",
    "\n",
    "This report includes all maps for which data were available, including those that were excluded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_coords = [-24,-10,4,18,32,52,64]\n",
    "\n",
    "bins = numpy.linspace(-5,5)\n",
    "\n",
    "\n",
    "figdir = os.path.join(output_dir,'figures/orig_map_overlays')\n",
    "\n",
    "if not os.path.exists(figdir):\n",
    "    os.mkdir(figdir)\n",
    "\n",
    "\n",
    "for hyp in [1,2,5,6,7,8,9]:\n",
    "    outfile = os.path.join(figdir,'hyp%d_orig_map_overlays.pdf'%hyp)\n",
    "    \n",
    "    if not os.path.exists(outfile) or overwrite:\n",
    "        print('making map overlay figure for hyp',hyp)\n",
    "        \n",
    "        # find all maps\n",
    "        hmaps = glob.glob(os.path.join(output_dir,'orig/*_*'))\n",
    "        \n",
    "        collection_ids = [os.path.basename(i) for i in hmaps]\n",
    "        collection_ids.sort()\n",
    "\n",
    "        fig, ax = plt.subplots(len(collection_ids),2,figsize=(len(collection_ids),140),gridspec_kw={'width_ratios': [2, 1]})\n",
    "        ctr=0\n",
    "\n",
    "        for collection_id in collection_ids:\n",
    "            teamID = collection_id.split('_')[1]\n",
    "            unthresh_img=os.path.join(output_dir,'orig/%s/hypo%d_unthresh.nii.gz'%(collection_id,hyp))\n",
    "            thresh_img=os.path.join(output_dir,'thresh_mask_orig/%s/hypo%d_thresh.nii.gz'%(collection_id,hyp))\n",
    "            \n",
    "            if not (os.path.exists(thresh_img) or os.path.exists(unthresh_img)):\n",
    "                print('skipping',teamID)\n",
    "                continue\n",
    "                \n",
    "            if not teamID in narps.complete_image_sets:\n",
    "                imagetitle = '%s (excluded)'%teamID\n",
    "            else:\n",
    "                imagetitle = teamID\n",
    "                \n",
    "            display = nilearn.plotting.plot_stat_map(unthresh_img, display_mode=\"z\", \n",
    "                    colorbar=True,title=collection_id,\n",
    "                                      cut_coords = cut_coords,axes = ax[ctr,0],cmap='gray')\n",
    "\n",
    "            with warnings.catch_warnings():  # ignore levels warning\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                display.add_contours(thresh_img, filled=False, alpha=0.7, levels=[0.5], colors='b') \n",
    "\n",
    "            masker=nilearn.input_data.NiftiMasker(mask_img=thresh_img)\n",
    "            maskdata = masker.fit_transform(unthresh_img)\n",
    "            if numpy.sum(maskdata)>0:  # check for empty mask\n",
    "                hist_result=ax[ctr,1].hist(maskdata,bins=bins)\n",
    "            ctr+=1\n",
    "        plt.savefig(outfile)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create histograms for in-mask values in unthresholded images\n",
    "\n",
    "These are only created for the images that were successfully registered and rectified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir = os.path.join(output_dir,'figures/unthresh_histograms')\n",
    "\n",
    "if not os.path.exists(figdir):\n",
    "    os.mkdir(figdir)\n",
    "\n",
    "for hyp in [1,2,5,6,7,8,9]:\n",
    "    outfile = os.path.join(figdir,'hyp%d_unthresh_histogram.pdf'%hyp)\n",
    "    \n",
    "    if not os.path.exists(outfile) or overwrite:\n",
    "        print('making figure for hyp',hyp)\n",
    "        unthresh_data,labels=get_masked_data(hyp,mask_img,output_dir,imgtype='unthresh',dataset='rectified')\n",
    "        fig, ax = plt.subplots(int(numpy.ceil(len(labels)/3)),3,figsize=(16,50))\n",
    "        \n",
    "        # make three columns - these are row and column counters\n",
    "        ctr_x=0\n",
    "        ctr_y=0\n",
    "        \n",
    "        for i,l in enumerate(labels):\n",
    "                ax[ctr_x,ctr_y].hist(unthresh_data[i,:],100)\n",
    "                ax[ctr_x,ctr_y].set_title(l)\n",
    "                ctr_y+=1\n",
    "                if ctr_y>2:\n",
    "                    ctr_y=0\n",
    "                    ctr_x+=1\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
