{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus meta-analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os,glob\n",
    "import numpy\n",
    "import nibabel\n",
    "import nilearn.plotting\n",
    "import nilearn.input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas \n",
    "from statsmodels.stats.multitest import multipletests\n",
    "#from utils import t_corr\n",
    "import scipy.stats\n",
    "from narps import Narps,NarpsDirs\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set an environment variable called NARPS_BASEDIR with location of base directory\n",
    "if 'NARPS_BASEDIR' in os.environ:\n",
    "    basedir = os.environ['NARPS_BASEDIR']\n",
    "else:\n",
    "    basedir = '/data'\n",
    "assert os.path.exists(basedir)\n",
    "\n",
    "narps = Narps(basedir,overwrite=False)\n",
    "narps.load_data()\n",
    "\n",
    "orig_dir = os.path.join(basedir,'orig')\n",
    "metadata_dir = os.path.join(basedir,'metadata')\n",
    "output_dir = narps.dirs.dirs['output']\n",
    "figure_dir = os.path.join(basedir,'figures')\n",
    "if not os.path.exists(figure_dir):\n",
    "    os.mkdir(figure_dir)\n",
    "template_img = narps.dirs.MNI_template\n",
    "mask_img = narps.dirs.MNI_mask\n",
    "\n",
    "figure_dir = os.path.join(basedir,'figures')\n",
    "results_dir = os.path.join(output_dir,'1sample_ttest')\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_corr(y,res_mean=None,res_var=None,Q=None):\n",
    "    \"\"\"\n",
    "    perform a one-sample t-test on correlated data\n",
    "    y = data (n observations X n vars)\n",
    "    res_mean = Common mean over voxels and results\n",
    "    res_var  = Common variance over voxels and results\n",
    "    Q = \"known\" correlation across observations (use empirical correlation based on maps)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Jeanette:\n",
    "    # This paper calculates the df for an F-test, so the chisquare bit we need is in there.  Your t-statistic will come from\n",
    "    # X = column of 1's (design matrix)\n",
    "\n",
    "    npts = y.shape[0]\n",
    "    X = numpy.ones((npts,1))\n",
    "\n",
    "    if res_mean is None:\n",
    "        res_mean = 0\n",
    "\n",
    "    if res_var is None:\n",
    "        res_var = 1\n",
    "  \n",
    "    if Q is None:\n",
    "        Q = numpy.eye(npts)\n",
    "\n",
    "    # R = I{n} - X(X'X)^{-1}X'\n",
    "    R = numpy.eye(npts) - X.dot(numpy.linalg.inv(X.T.dot(X))).dot(X.T)\n",
    "\n",
    "    \n",
    "    VarMean = res_var * X.T.dot(Q).dot(X) / npts**2\n",
    "\n",
    "    # T  =  mean(y,0)/s-hat-2\n",
    "    # use diag to get s_hat2 for each variable \n",
    "    T = (numpy.mean(y,0)-res_mean)/numpy.sqrt(VarMean)*numpy.sqrt(res_var) + res_mean\n",
    "\n",
    "    # # *If* variance were estimated voxelwise on correlated data, the DF would follow \n",
    "    # # this expression = v = tr(RQ)^2/tr(RQRQ)\n",
    "    # df = (numpy.trace(R.dot(Q))**2)/numpy.trace(R.dot(Q).dot(R).dot(Q))\n",
    "    # p = 1 - scipy.stats.t.cdf(T,df=df)\n",
    "    \n",
    "    # Assuming variance is estimated on whole image\n",
    "    df = numpy.Inf\n",
    "    p  = 1 - scipy.stats.norm.cdf(T)\n",
    "    \n",
    "    return(T,df,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running ttests for hypothesis 1\n",
      "-0.30518347 2.6205375 0.44532649197487184\n",
      "0 voxels significant at FDR corrected p<.05\n",
      "running ttests for hypothesis 2\n",
      "-0.041382026 2.614344 0.5236263795086465\n",
      "2264 voxels significant at FDR corrected p<.05\n",
      "running ttests for hypothesis 5\n",
      "0.06092181 2.8584912 0.30649318210875587\n",
      "0 voxels significant at FDR corrected p<.05\n",
      "running ttests for hypothesis 6\n",
      "0.010589921 1.547799 0.17436821472132172\n",
      "0 voxels significant at FDR corrected p<.05\n",
      "running ttests for hypothesis 7\n",
      "0.041057035 2.787075 0.4427001295604738\n",
      "0 voxels significant at FDR corrected p<.05\n",
      "running ttests for hypothesis 8\n",
      "-0.032200698 1.5379735 0.3189629997799937\n",
      "0 voxels significant at FDR corrected p<.05\n",
      "running ttests for hypothesis 9\n",
      "0.031529248 1.5273856 0.14699694776911013\n",
      "0 voxels significant at FDR corrected p<.05\n"
     ]
    }
   ],
   "source": [
    "masker = nilearn.input_data.NiftiMasker(mask_img=mask_img)\n",
    "\n",
    "for hyp in [1,2,5,6,7,8,9]:\n",
    "    print('running ttests for hypothesis',hyp)\n",
    "    maps = glob.glob(os.path.join(output_dir,'zstat/*/hypo%d_unthresh.nii.gz'%hyp))\n",
    "    maps.sort()\n",
    "    data = masker.fit_transform(maps)\n",
    "    \n",
    "    # get estimated mean, variance, and correlation for t_corr\n",
    "    img_mean = numpy.mean(data)\n",
    "    img_var = numpy.mean(numpy.var(data,1))\n",
    "    cc=numpy.corrcoef(data)\n",
    "    print(img_mean,img_var,numpy.mean(cc[numpy.triu_indices_from(cc,1)]))\n",
    "    # perform t-test\n",
    "    tvals = numpy.zeros(data.shape[1])\n",
    "    pvals = numpy.zeros(data.shape[1])\n",
    "    dfs = numpy.zeros(data.shape[1])\n",
    "    tvals,dfs,pvals=t_corr(data,res_mean=img_mean,res_var=img_var,Q=cc)\n",
    "    \n",
    "    # move back into image format\n",
    "    timg = masker.inverse_transform(tvals)\n",
    "    timg.to_filename(os.path.join(results_dir,'hypo%d_t.nii.gz'%hyp))\n",
    "    pimg = masker.inverse_transform(1-pvals)\n",
    "    pimg.to_filename(os.path.join(results_dir,'hypo%d_1-p.nii.gz'%hyp))\n",
    "    fdr_results = multipletests(pvals[0,:],0.05,'fdr_tsbh')\n",
    "    print(\"%d voxels significant at FDR corrected p<.05\"%numpy.sum(fdr_results[0]))\n",
    "    fdrimg = masker.inverse_transform(1 - fdr_results[1])\n",
    "    fdrimg.to_filename(os.path.join(results_dir,'hypo%d_1-fdr.nii.gz'%hyp))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(7,1,figsize=(12,24))\n",
    "thresh=0.95\n",
    "hypnums=[1,2,5,6,7,8,9]\n",
    "cut_coords = [-24,-10,4,18,32,52,64]\n",
    "hypotheses= {1:'+gain: equal indiff',\n",
    "            2:'+gain: equal range',\n",
    "            3:'+gain: equal indiff',\n",
    "            4:'+gain: equal range',\n",
    "            5:'-loss: equal indiff',\n",
    "            6:'-loss: equal range',\n",
    "            7:'+loss: equal indiff',\n",
    "            8:'+loss: equal range',\n",
    "            9:'+loss:ER>EI'}\n",
    "\n",
    "for i,hyp in enumerate(hypnums):\n",
    "    pmap = os.path.join(output_dir,'unthresh_randomise/hypo%d_clustere_corrp_tstat1.nii.gz'%hyp)\n",
    "    tmap =  os.path.join(output_dir,'unthresh_randomise/hypo%d_tstat1.nii.gz'%hyp)\n",
    "    pimg = nibabel.load(pmap)\n",
    "    timg = nibabel.load(tmap)\n",
    "    threshdata = (pimg.get_data()>thresh)*timg.get_data()\n",
    "    threshimg = nibabel.Nifti1Image(threshdata,affine=timg.affine)\n",
    "    nilearn.plotting.plot_stat_map(threshimg, threshold=0.1, display_mode=\"z\", \n",
    "                colorbar=True,title='hyp %d:'%hyp+hypotheses[hyp],vmax=8,cmap='jet',\n",
    "                                  cut_coords = cut_coords,axes = ax[i])\n",
    "\n",
    "plt.savefig(os.path.join(figure_dir,'consensus_map.pdf'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run models with regressors for analysis features\n",
    "\n",
    "Include the following regressors:\n",
    "- was fmriprep used?\n",
    "- was motion regression used?\n",
    "- was confound regression (compcor or other) used?\n",
    "- was a separate RT regressor used?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design files\n",
    "\n",
    "metadata_file = '/Users/poldrack/data_unsynced/NARPS/analysis_pipelines_SW.xlsx'\n",
    "metadata = pandas.read_excel(metadata_file,header=1)\n",
    "metadata['collectionID']=None\n",
    "for i in metadata.index:\n",
    "    metadata.loc[i,'collectionID']= '%s_%s'%(metadata.loc[i,'NV_collection_string'].strip(),metadata.loc[i,'teamID'].strip())\n",
    "metadata.index = metadata.collectionID\n",
    "metadata['used_fmriprep_data'] = (metadata['used_fmriprep_data']=='Yes').astype('int')*2 - 1\n",
    "metadata.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = ['used_fmriprep_data']\n",
    "md = metadata[columns_to_use]\n",
    "\n",
    "analysis_dir = os.path.join(output_dir,'unthresh_randomise_fmriprep')\n",
    "\n",
    "# just need to do it for one hypothesis since it's same for all\n",
    "hyp=1\n",
    "in_file=os.path.join(output_dir,'unthresh_concat/hypo%d.nii.gz'%hyp)\n",
    "label_file = os.path.join(output_dir,'unthresh_concat/labels_hypo%d.txt'%hyp)\n",
    "labels = pandas.read_csv(label_file,header=None)\n",
    "labels.columns = ['NV_collection_string']\n",
    "labels.index = labels.NV_collection_string\n",
    "labels['constant']=1\n",
    "nlabels=labels.shape[0]\n",
    "labels = labels.merge(md,left_index=True,right_index=True)\n",
    "assert labels.shape[0] == nlabels  # make sure we didn't lose any!\n",
    "del labels['NV_collection_string']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write fsl design file\n",
    "designfile = os.path.join(analysis_dir,'design.txt')\n",
    "with open(designfile,'w') as f:\n",
    "    f.write('/NumWaves %d\\n'%labels.shape[1])\n",
    "    f.write('/NumPoints %d\\n/Matrix\\n'%labels.shape[0])\n",
    "    f.write(labels.to_string(header=False,index=False))\n",
    "\n",
    "confile = os.path.join(analysis_dir,'design.con')\n",
    "with open(confile,'w') as f:\n",
    "    f.write('/NumWaves 2\\n')\n",
    "    f.write('/NumPoints 4\\n/Matrix\\n')\n",
    "    f.write('1\\t0\\n')\n",
    "    f.write('-1\\t0\\n')\n",
    "    f.write('0\\t1\\n')\n",
    "    f.write('0\\t-1\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyp in [5]:\n",
    "    rand = fsl.Randomise(in_file=os.path.join(output_dir,'unthresh_concat/hypo%d.nii.gz'%hyp), \n",
    "                         mask = mask_img,\n",
    "                        base_name=os.path.join(output_dir,'unthresh_randomise_fmriprep/hypo%d'%hyp),\n",
    "                        num_perm=2500,c_thresh=3,\n",
    "                        var_smooth=True,design_mat=designfile,\n",
    "                        tcon=confile)\n",
    "    print(rand.cmdline)\n",
    "    rand.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cope_desc = {1:'pos',2:'neg',3:''}\n",
    "for cope in range(1,5):\n",
    "    for i,hyp in enumerate(hypnums):\n",
    "        pmap = os.path.join(output_dir,'unthresh_randomise_fmriprep/hypo%d_clustere_corrp_tstat%d.nii.gz'%(hyp,cope))\n",
    "        tmap =  os.path.join(output_dir,'unthresh_randomise_fmriprep/hypo%d_tstat%d.nii.gz'%(hyp,cope))\n",
    "        pimg = nibabel.load(pmap)\n",
    "        timg = nibabel.load(tmap)\n",
    "        threshdata = (pimg.get_data()>thresh)*timg.get_data()\n",
    "        threshimg = nibabel.Nifti1Image(threshdata,affine=timg.affine)\n",
    "        nilearn.plotting.plot_stat_map(threshimg, threshold=0.1, display_mode=\"z\", \n",
    "                    colorbar=True,title='hyp %d:'%hyp+hypotheses[hyp],vmax=8,cmap='jet',\n",
    "                                      cut_coords = cut_coords,axes = ax[i])\n",
    "\n",
    "    plt.savefig(os.path.join(figure_dir,'consensus_map_cope%d.pdf'%cope))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
